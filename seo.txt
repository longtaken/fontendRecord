
SEO就是优化除了花钱推广之外的排名顺序，学习百度搜索引擎的SEO规则，让网站在百度搜索的时候排名更前


SEO
第一章
SEO是针对网站在搜索引擎里的自然排名，不可能做到把花钱的网站挤到后面
花钱推广一般按照用户点击次数付钱，如果转化率不高则容易亏本。SEO则无需付钱





第二章
域名不能使用曾经被搜索引擎惩罚过的域名或已经被他人丢弃的域名
可以在搜索引擎中搜索域名，如果找不到任何相关内容则说明域名从未被使用。
如果网站打开速度慢，搜索引擎会认为用户体验不好，不会给好排名。
如果网站经常打不开，搜索引擎也不会给好的排名
织梦程序做SEO比较方便

关键词
做网站的SEO其实是做关键词的排名
关键词从概念上
	目标关键词
		目标关键词的搜索量比长尾关键词大一点，目标关键词都是放在网站的首页标题中，是用首页来优化的。代表了整个网站的主题和思想，网站整个内容都是围绕目标关键词展开（首页

指index页）
		基本都放在首页的标题中。
		
	长尾关键词
		网站上非目标关键词但可以带来搜索流量的关键词称为长尾关键词。（关键词放在网站首页即为目标关键词；放在内页即为长尾关键词）。一般为关键词的衍生词（如“减肥”为目标

关键词，有人通过“太胖了”到了网站某个页面，“太胖了”即为长尾关键词）。长尾关键词搜索量较少，但如果每篇文章都含有一个或几个长尾关键词，效果将会很好

	相关关键词
		相关关键词是与目标关键词或长尾关键词相关的关键词。（比如“运动”与“减肥”，运动一般与减肥一起提及。）。相关关键词的作用就是辅助目标关键词或长尾关键词

从页面上
	首页关键词
		首页关键词是放在首页的关键词，等同于目标关键词
	
	栏目页关键词		
	内容页关键词
		栏目内容页关键词放在内页，等同于长尾关键词

从目的性上
	直接性关键词
	营销性关键词

关键词可以具有多重身份

百度指数，有多少人在搜索一个关键词





第三章
锚文本能告诉搜索引擎该超链接指向的页面主要讲什么内容，涵盖所指向页面的主题。
带有文字的超链接即为锚文本，会让搜索引擎更明确地指向页面的主题有利网站排名
大部分通过SEO做网站的排名都是围绕内链外锚来操作

友情链接，A网站通过一个链接指向B网站，B网站也通过一个链接指向A。

导出链接，A网站有一个链接指向B网站，这个链接叫做导出链接。

导入链接，B网站指向A。

单向链接 A指向B，B没有指向A

双向链接 AB互指

单向链接会使搜索引擎给的权重更高


网站源码
title为网站标题
keywords是一个定位，表示该页面主要围绕什么来介绍
description告诉搜索引擎该页面主要讲什么





第四章
SEO的角度，搜索不到的文章即为原创
采集就是把文章照搬到自己网站，可能会被搜索引擎惩罚
伪原创是照搬的时候改一点文字
推荐原创，采集风险大，可以采用伪原创文章

百度指数高，搜索的人多，竞争度也高
可以选择百度指数低竞争度低 定向度高的词，有利有弊
太高指数导致搜索第一页没有出现自己的产品
选择关键词可以按照：竞争度高+中等竞争度+低竞争度


百度的分词就是把某些词从标题里分出来，通过切词、分词，然后重新组合起来
首先确定3个最想做的关键词，其他关键词也不排除，然后通过添加组合的方式组合成一个长标题，长标题里包含这些关键词
“的”，“了”，“吗” 等常用副词，百度可以忽略
网站标题除了尽量通顺并包含所有要做排名的目标关键词外，还要注意把排名难度大的关键词写在标题前面，这样更有利于排名。网站标题中一段话完全连在一起比被分散开来更容易排名靠前，所以排

名难度较高的词尽量不要拆分

关键词密度是指关键词在网页上出现的总次数与其他文字的比例，一般用百分比表示。
一般对关键词密度高低没有太多要求，自然合理即可。如果堆积关键词，搜索引擎局的你在可以做SEO，而并未从用户角度考虑网站，那么你的网站容易被惩罚。
关键词密度一般保持在1%-10%或2%-8%

关键词密度查询可以直接使用站长工具查询

文章更新频率在创建开始尽量保持每天更新一篇原创文章，一两个月后如果网站情况良好可以减少频率。
文章不写非法反动的内容。





第五章
外链是指别的网站导入到自己网站的链接。导入链接对于网站优化来说是非常重要的一个过程
导入链接的质量（即导入链接所在页面的权重）直接决定了我们网站在搜索引擎中的权重。
SEO很重要一点是通过外链提高知名度。

a链接的rel="nofollow" 告诉搜索引擎这个超链接不是有意要指向目标网站，不要传递权重给它。

使用软文外链 博客外链（百度一般不收录QQ空间的文章） 注意论坛是否屏蔽百度蜘蛛，屏蔽对排名无用

购买外链就是给对方钱，让对方加上一个单向链接指向自己的网站。A5网站可以购买外链 

PR值（PR10最高，最低PR0）为谷歌提出，百度排名没有直接影响
要根据网站关键词的竞争难度来购买链接
购买时间一般在2-3个月以上
购买链接要注意销售链接的网站销售了多少外链，如果过多最好不要购买

视频做外链（增加网址曝光率）

软件做外链（天空下载，多特软件站，太平洋下载等）

黑链，在网站中看不到

外链注意事项
1，外链数量要有比较平稳的增长度，每天做的外链数量要大概相同。
2，在论坛上做外链，页面需要被收录外链才有效
3，一些论坛需要注册账号后才能浏览帖子，这样搜索引擎也无法收录
4，有些网站被百度K掉，完全不收录。淘宝网等会主动屏蔽百度蜘蛛，发外链也没用
5，外链质量数量都要好





第六章
链轮是指建立大量的独立站点，这些独立站点通过单向的、有策略、有计划紧密的链接指向要优化的目标网站，以提升目标网站在搜索引擎中的排名。
链轮模式需要大量网站，由团队配合去做

domain 外链查询

网站收录量是指搜索引擎收录一个网站的页面数量。网站收录量在SEO中也有着极其重要的意义。

查询收录量  site:网址
雅虎、搜搜等搜索引擎统一使用site指令

20%的以上的文章被收录算情况良好
文章被收录不一定立刻显示出来

（百度在技术上达不到谷歌搜索引擎的高水平）

更新文章需要考虑长尾关键词
长尾关键词要包含在文章页标题里
长尾关键词要点
1，自然地融入长尾关键词或分词，增加文章的关键词密度
2，写到其他文章的长尾关键词时，做一个超链接指向改文章
3，文章标题要包含选定的长尾关键词
4，出现长尾关键词可以加粗，可做可不做


次导航是辅助导航，一般在页面底部。
次导航又叫全站链接，即全站每个页面都做链接指向首页，集中力量提高首页关键词的权重，优化排名。目的是为了增加内链。

错误链接是根本不存在的链接
死链接是原本访问正常，后因网站变故而不能访问的链接
死链接不能太多，否则搜索引擎对网站印象不好（一般网站改版会出现很多死链接，可以采用robots文件把搜索引擎屏蔽掉）

中大型网站做自定义的404页面是有必要的（但对SEO排名不太重要）

Robots协议（也称为爬虫协议、机器人协议等）的全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。

robots.txt文件是一个文本文件，robots.txt是一个协议，而不是一个命令。robots.txt是搜索引擎中访问网站的时候要查看的第一个文件。robots.txt文件告诉蜘蛛程序在服务器上什么文件是可以被

查看的。
当一个搜索蜘蛛访问一个站点时，它会首先检查该站点根目录下是否存在robots.txt，如果存在，搜索机器人就会按照该文件中的内容来确定访问的范围；如果该文件不存在，所有的搜索蜘蛛将能够访

问网站上所有没有被口令保护的页面。
百度官方建议，仅当您的网站包含不希望被搜索引擎收录的内容时，才需要使用robots.txt文件。如果您希望搜索引擎收录网站上所有内容，请勿建立robots.txt文件。
如果将网站视为酒店里的一个房间，robots.txt就是主人在房间门口悬挂的“请勿打扰”或“欢迎打扫”的提示牌。这个文件告诉来访的搜索引擎哪些房间可以进入和参观，可能涉及住户及访客的隐私

而不对搜索引擎开放。但robots.txt不是命令，也不是防火墙，如同守门人无法阻止窃贼等恶意闯入者

首页网址后加robots.txt即可打开网站的robots文件

robots对中大型网站来说比较重要

百度对robots.txt是有反应的，但比较慢，在减少禁止目录抓取的同时也减少了正常目录的抓取。
原因应该是入口减少了，正常目录收录需要后面再慢慢增加。
Google对robots.txt反应很到位，禁止目录马上消失了，部分正常目录收录马上上升了。/comment/目录收录也下降了，还是受到了一些老目标减少的影响。
搜狗抓取呈现普遍增加的均势，部分禁止目录收录下降了。
总结一下：Google似乎最懂站长的意思，百度等其它搜索引擎只是被动的受入口数量影响了。
淘宝封杀
2008年9月8日，淘宝网宣布封杀百度爬虫，百度忍痛遵守爬虫协议。因为一旦破坏协议，用户的隐私和利益就无法得到保障，搜索网站就谈不到人性关怀。
京东封杀
2011年10月25日，京东商城正式将一淘网的搜索爬虫屏蔽，以防止一淘网对其的内容抓取。

h1-h6越靠前越重要，h1标签是要告诉搜索引擎蜘蛛该标签里包含的内容相对整个网页来讲是最重要的
title标签是告诉搜索引擎该网页的标题是什么

百度抓取图片
1：通过文章的内容、title标签等判断抓取
2：通过图片专有的alt标签


权重是指搜索引擎赋予网站的权威值，是对网站权威的评估评价。权重越高，在搜索引擎所占的分量越大排名越好

PR值0-10，PR与权重不能完全等同

nofollow标签
意义是告诉搜索引擎“不要追踪此网页上的链接”或“不要追踪此特定链接”
<a src="obj" rel="nofollow"></a>设置之后告诉搜索引擎不要传递权重给这个外链
设置之后搜索引擎还是会抓取“obj”，只是频率和概率比较低

源代码
搜索引擎不喜欢冗长繁杂的代码
代码中需要具备title标签和描述标签，H1标签的使用
符合SEO的开源程序，代码结构良好

301重定向，输入baidu.com，跳转www.baidu.com则表示该网站做了301重定向
（把旧域名权重传递到新域名）

www.a.com/a/a.html 目录浅更利于SEO的优化

黑链 搜索引擎蜘蛛访问文章是通过查看源代码，在源码做了黑链，就起到外链的作用





第七章
动态地址 带有“？”“*”或者以.sp .php 结尾
静态地址 以.htm .html结尾
静态地址更有利于SEO，文章页尽可能用静态地址

跳出率 用户通过搜索引擎查找一个网站并点击进入，但只浏览一个页面就离开，不点击任何链接。如果100个人有50个人这样搜索，那么跳出率就是50%。对大部分网站跳出率越低越好

PV即页面浏览量，是指用户在网站浏览的页面数量。在同一类网站中，PV越高搜索引擎认为网站越受欢迎


百度快照
当无法打开某个搜索结果或者打开速度特别慢，可以通过百度快照查看网站内容

更换空间IP 
虚拟主机里同一个IP下会放大量网站，虽然百度不承认IP连坐的情况，但IP下如果很多网站作弊，那么百度会认识你的网站不好

title标签最好不好经常改动

SEO排名
上升下降几名是正常的，如果突然掉到很靠后的地方或者找不到，可能网站被降权。


发外链，更新文章，美化标题
（文章的更新 外链 执行力）
百度推广的广告

分析网站的具体做法
收录量
	site:www.site.com
外链
	domain:www.site.com	（外链做法，门户网站软文、友情链接、问答类、论坛外链、购买高权重链接、站群）
	站群是指大量网站互相做链接，互相提升权重模式，垄断某些关键词
PR和权重
	PR查询 百度权重查询




第八章
搜索引擎竞价推广





第九章
关键词添加及创意撰写（百度推广相关）





第十章
百度推广投放与统计报告





第十一章
网盟推广
网盟是网站的广告联盟，是精准投放广告的一种






附注：
Robots协议
Robots协议用来告知搜索引擎哪些页面能被抓取，哪些页面不能被抓取；可以屏蔽一些网站中比较大的文件，如：图片，音乐，视频等，节省服务器带宽；可以屏蔽站点的一些死链接。方便搜索引擎抓

取网站内容；设置网站地图连接，方便引导蜘蛛爬取页面。

User-agent: * 这里的*代表的所有的搜索引擎种类，*是一个通配符
Disallow: /admin/ 这里定义是禁止爬寻admin目录下面的目录
Disallow: /require/ 这里定义是禁止爬寻require目录下面的目录
Disallow: /ABC/ 这里定义是禁止爬寻ABC目录下面的目录
Disallow: /cgi-bin/*.htm 禁止访问/cgi-bin/目录下的所有以".htm"为后缀的URL(包含子目录)。
Disallow: /*?* 禁止访问网站中所有包含问号 (?) 的网址
Disallow: /.jpg$ 禁止抓取网页所有的.jpg格式的图片
Disallow:/ab/adc.html 禁止爬取ab文件夹下面的adc.html文件。
Allow: /cgi-bin/　这里定义是允许爬寻cgi-bin目录下面的目录
Allow: /tmp 这里定义是允许爬寻tmp的整个目录
Allow: .htm$ 仅允许访问以".htm"为后缀的URL。
Allow: .gif$ 允许抓取网页和gif格式图片
Sitemap: 网站地图 告诉爬虫这个页面是网站地图



例1. 禁止所有搜索引擎访问网站的任何部分
User-agent: *
Disallow: /
实例分析：淘宝网的 Robots.txt文件
User-agent: Baiduspider
Disallow: /
User-agent: baiduspider
Disallow: /
很显然淘宝不允许百度的机器人访问其网站下其所有的目录。
例2. 允许所有的robot访问 (或者也可以建一个空文件 “/robots.txt” file)
User-agent: *
Allow:　/
例3. 禁止某个搜索引擎的访问
User-agent: BadBot
Disallow: /
例4. 允许某个搜索引擎的访问
User-agent: Baiduspider
allow:/
例5.一个简单例子
在这个例子中，该网站有三个目录对搜索引擎的访问做了限制，即搜索引擎不会访问这三个目录。
需要注意的是对每一个目录必须分开声明，而不要写成 “Disallow: /cgi-bin/ /tmp/”。
User-agent:后的*具有特殊的含义，代表“any robot”，所以在该文件中不能有“Disallow: /tmp/*” or “Disallow:*.gif”这样的记录出现。
User-agent: *
Disallow: /cgi-bin/
Disallow: /tmp/
Disallow: /~joe/
Robot特殊参数：
允许 Googlebot：
如果您要拦截除Googlebot以外的所有漫游器不能访问您的网页，可以使用下列语法：
User-agent:
Disallow: /
User-agent: Googlebot
Disallow:
Googlebot 跟随指向它自己的行，而不是指向所有漫游器的行。
“Allow”扩展名：
Googlebot 可识别称为“Allow”的 robots.txt 标准扩展名。其他搜索引擎的漫游器可能无法识别此扩展名，因此请使用您感兴趣的其他搜索引擎进行查找。“Allow”行的作用原理完全与“Disallow
”行一样。只需列出您要允许的目录或页面即可。
您也可以同时使用“Disallow”和“Allow”。例如，要拦截子目录中某个页面之外的其他所有页面，可以使用下列条目：
User-agent: Googlebot
Allow: /folder1/myfile.html
Disallow: /folder1/
这些条目将拦截 folder1 目录内除 myfile.html 之外的所有页面。
如果您要拦截 Googlebot 并允许 Google 的另一个漫游器（如 Googlebot-Mobile），可使用”Allow”规则允许该漫游器的访问。例如：
User-agent: Googlebot
Disallow: /
User-agent: Googlebot-Mobile
Allow:
使用 * 号匹配字符序列：
您可使用星号 (*) 来匹配字符序列。例如，要拦截对所有以 private 开头的子目录的访问，可使用下列条目：　User-Agent: Googlebot
Disallow: /private*/
要拦截对所有包含问号 (?) 的网址的访问，可使用下列条目：
User-agent: *
Disallow: /*?*
使用 $ 匹配网址的结束字符
您可使用 $字符指定与网址的结束字符进行匹配。例如，要拦截以 .asp 结尾的网址，可使用下列条目：　User-agent: Googlebot
Disallow: /*.asp$
您可将此模式匹配与 Allow 指令配合使用。例如，如果 ? 表示一个会话 ID，您可排除所有包含该 ID 的网址，确保 Googlebot 不会抓取重复的网页。但是，以 ? 结尾的网址可能是您要包含的网页版
本。在此情况下，可对 robots.txt 文件进行如下设置：
User-agent: *
Allow: /*?$
Disallow: /*?
Disallow: / *?
一行将拦截包含 ? 的网址（具体而言，它将拦截所有以您的域名开头、后接任意字符串，然后是问号 (?)，而后又是任意字符串的网址）。
Allow: /*?$ 一行将允许包含任何以 ? 结尾的网址（具体而言，它将允许包含所有以您的域名开头、后接任意字符串，然后是问号 (?)，问号之后没有任何字符的网址）。
尽管robots.txt已经存在很多年了，但是各大搜索引擎对它的解读都有细微差别。Google与百度都分别在自己的站长工具中提供了robots工具。如果您编写了robots.txt文件，建议您在这两个工具中都

进行测试，因为这两者的解析实现确实有细微差别



Robots.txt文件主要是限制整个站点或者目录的搜索引擎访问情况，而Robots Meta标签则主要是针对一个个具体的页面。和其他的META标签（如使用的语言、页面的描述、关键词等）一样，Robots 

Meta标签也是放在页面中，专门用来告诉搜索引擎ROBOTS如何抓取该页的内容。
Robots Meta标签中没有大小写之分，name=”Robots”表示所有的搜索引擎，可以针对某个具体搜索引擎写为name=”BaiduSpider”。content部分有四个指令选项：index、noindex、follow、nofollow

，指令间以“,”分隔。
index指令告诉搜索机器人抓取该页面；
follow指令表示搜索机器人可以沿着该页面上的链接继续抓取下去；
Robots Meta标签的缺省值是index和follow，只有inktomi除外，对于它，缺省值是index、nofollow。

